# Sequence Recommendation

## 시계열 추천 시스템 (Sequential Recommendation)
- 정의: 시계열 추천 시스템은 사용자-항목 상호작용 시퀀스를 바탕으로 사용자가 다음에 상호작용할 항목(구매, 클릭, 시청 등)을 예측하는 시스템입니다.
- 목적: 시간에 따른 사용자 행동 패턴을 학습하여 보다 정교한 맞춤 추천을 제공하는 것이 목표입니다.

## RNN 기반 모델 (예: GRU, GRU4Rec)
- GRU (Gated Recurrent Unit)
  - 시계열 데이터를 처리하기 위해 사용되는 순환 신경망(RNN)의 한 종류로, 입력 시퀀스의 길이가 길어질수록 시간에 따라 정보를 전달하는 것이 어려워지는 한계가 있습니다.
- GRU4Rec
  - 시계열 추천에 GRU를 적용한 모델로, 이전 사용 기록을 바탕으로 다음에 추천할 항목을 예측합니다. 예를 들어, "컴퓨터"를 구매한 사용자는 "마우스"를 추천받을 가능성이 높습니다.

## Self-Attentive Sequential Recommendation (SASRec)
- Self-Attention 기법: 트랜스포머(Transformer)의 self-attention 메커니즘을 기반으로, 시퀀스 내 각 항목 간의 관계를 학습합니다.
- SASRec 특징
  - 시퀀스를 key, query, value로 변환하여 각 항목 간의 상호작용을 계산합니다.
  - 단방향 모델로, 이전 항목만을 사용하여 다음 항목을 예측합니다.
  - 임베딩 레이어는 학습 가능한 위치 임베딩을 포함하여 항목 간 순서 정보를 반영합니다.
- 예측 레이어: 현재까지의 컨텍스트와 다음 항목 간의 유사도를 높이고, 부정적 샘플과의 유사도는 낮추는 방향으로 학습됩니다.


## BERT 기반 모델 (BERT4Rec)
- BERT4Rec 개요
  - BERT의 양방향 트랜스포머 구조를 활용하여, 사용자의 상호작용 시퀀스에서 다음 항목을 예측합니다. 사용자가 시퀀스의 앞뒤 정보를 모두 참고해 예측이 가능하도록 합니다.
- 마스킹(Masking) 기법
  - BERT의 마스킹 방식을 적용하여, 훈련 시에는 시퀀스 중 일부를 마스킹하고 해당 항목을 예측하도록 학습합니다.
  - **클로즈 태스크(Cloze Task)** 를 적용하여 중간에 누락된 항목을 채우는 방식으로 학습됩니다.
- BERT와 BERT4Rec의 차이
  - BERT는 문장 내의 문맥을 이해하도록 사전 훈련된 모델이지만, BERT4Rec은 사용자-항목 상호작용 데이터를 사용해 엔드투엔드 방식으로 학습됩니다.
  - BERT4Rec은 세션 기반이 아닌 사용자의 전체 시퀀스를 단일 시퀀스로 간주합니다.

## SASRec과 BERT4Rec 비교
- SASRec
  - 단일 헤드(single-head) attention을 사용하고, 다음 항목만 마스킹하여 예측에 사용합니다.
  - 각 시간 단계에서 이전 항목만을 기반으로 손실을 계산합니다.
- BERT4Rec
  - 다중 헤드(multi-head) attention을 사용하며, 랜덤하게 마스크된 항목을 예측하는 방식으로 학습됩니다.
  - 전체 시퀀스에 대해 마스킹된 항목 예측 손실을 계산합니다.

## 평가지표
- HR@K (Hit Ratio at K)
  - 상위 K개의 추천 리스트에서 실제로 사용자가 상호작용할 항목이 포함된 비율을 측정합니다.
- NDCG@K (Normalized Discounted Cumulative Gain)
  - 상위 K개 추천 항목의 순서에 따라 관련성이 높은 항목이 상위에 위치할수록 높은 점수를 부여하는 지표입니다.
- MRR (Mean Reciprocal Rank)
  - 사용자가 상호작용할 첫 번째 관련 항목의 순위의 역수를 평균하여 측정합니다.